---
title: 2025：当AI长出"手"和"脑"——我们正见证一个波澜壮阔的智能体元年
date: 2026-02-16 17:10:00
tags: [AI, Agent, 智能体, 2025, DeepSeek, Manus, OpenClaw, 千问, 综述]
categories: 技术
---

你有没有发现一个有趣的变化？

2025年的AI，和往年那个只会"聊天"的家伙，已经不太一样了。

回想一下：以前我们问AI"明天北京天气怎么样"，它告诉你"晴，15到22度"，对话就结束了。干净利落，但也止步于此。

但现在呢？如果你说"帮我安排一次北京三日游，预算3000元"，它会直接帮你订好机票、筛选酒店、规划每天的行程路线——**它不再只是动嘴皮子，而是真正开始动手做事了**。

从"说话"到"做事"，这一步看似简单，却是AI发展史上的一次质变。而这一切的转折，要从2025年春节前那个"让AI学会慢思考"的革命性突破说起。

<!-- more -->

---

## 一、DeepSeek R1：点燃智能体革命的"第一把火"

2025年春节前夕，DeepSeek R1的横空出世，像一颗重磅炸弹引爆了整个科技圈。

但让它真正与众不同的，不是动辄千亿级的参数规模，而是它**学会了像人类一样"慢思考"**。

### 快思考 vs 慢思考：AI的"顿悟时刻"

传统的大模型就像一个反应超快的学生：你问"1+1等于几"，它脱口而出"2"。但如果你问"为什么天空是蓝色的"，它可能就要停顿一下、推理一番、甚至"查查资料"才能给出答案。

这就是诺贝尔奖得主丹尼尔·卡尼曼所说的"快思考"与"慢思考"的区别。

DeepSeek R1引入的**思维链（Chain-of-Thought，简称CoT）**技术，让AI第一次拥有了"自言自语"式的思考能力。

想象一下这样的场景：当你抛给它一道复杂的数学题，R1会在屏幕上一行一行展示它的"心路历程"——

> *"首先，我需要理解题目的已知条件..."*
> *"然后，设x为未知数..."*
> *"根据第二个条件，我可以列出方程..."*

这种一步一步、层层递进的推理过程，不仅让它的准确率大幅提升，更重要的是——**我们第一次能"看见"AI是怎么想的**。

### 纯强化学习：让AI在黑暗中自己找到光

**更令人惊叹的是，R1采用了纯强化学习的方式训练。**

这是什么概念？打个比方：就像把一个孩子扔进一个漆黑的房间，没有老师手把手教，没有标准答案可以参考，他只能通过不断试错、不断摸索，最终自己找到门在哪里。

R1就是这样"自学成才"的。

**但更震撼的是它的成本。**

DeepSeek披露，R1的训练成本仅为**560万美元**——这还不到OpenAI等美国公司训练同等水平模型成本的**1/10**。而且DeepSeek选择**完全开源**（MIT许可证），这意味着全球的开发者和企业都可以免费使用、修改、甚至商业化。

这一"组合拳"直接在全球科技股引发了"地震"：Nvidia股价单日暴跌17%，市值蒸发近6000亿美元。投资者突然意识到：AI的护城河，可能并没有想象中那么深。

这一突破证明了一个重要结论：**AI的推理能力可以通过"自我学习"获得，而不需要依赖昂贵的人工标注数据，也不需要天价算力投入。**这对整个行业来说，无异于打开了一扇新的大门。

### 为什么说这是智能体时代的"发令枪"？

因为**CoT让AI从"答题机器"进化成了"解题者"**。

以前的AI搜索，说白了就是在海量文档里机械地匹配关键词。但现在不一样了——它可以真正"理解"你的问题，然后主动规划解题路径：先搜索A，再基于A的结果去搜索B，最后综合分析给出答案。

这种"会思考"的能力，正是智能体能够自主执行复杂任务的**前提条件**。

如果说之前的AI是"你问我答"的问答机器，那么从R1开始，AI正式迈入了"你说我做"的智能体时代。

---

## 二、从"会思考"到"会动手"：Manus和OpenClaw为什么能火？

如果说DeepSeek R1让AI长出了"大脑"，那么2025年3月发布的Manus，以及2026年春节前一夜爆红的OpenClaw，则让AI真正长出了"手"和"脚"。

但问题来了：为什么偏偏是它们？为什么是在这个时间点？

答案藏在两个关键条件的成熟之中。

### 条件一：代码生成能力的"核爆"

2025年，以Anthropic为代表的科技公司在**代码生成**领域取得了突破性进展。

Claude Code的发布，标志着一个新时代的开启：AI不再只是帮你"写代码"，而是能够——

- 在终端中自主执行复杂的工程任务
- 自主读写文件、运行程序、调试报错
- 甚至根据设计稿一键生成可交互的Web应用

**更关键的是Anthropic推出的"Computer Use"能力。**

这不是简单的API调用，而是让AI真正"看到"屏幕、"操作"鼠标键盘。它可以：
- 截取屏幕截图，理解当前界面状态
- 精准点击按钮、输入文字、滚动页面
- 在浏览器里自主导航、填写表单、下载文件
- 甚至像人类一样"观察-思考-行动"的循环

这意味着什么？

**意味着AI终于拥有了"现场制造工具"的能力，而且是视觉驱动的。**

举个例子：你让Manus"分析一只股票并生成报告"。它不会傻傻地等待指令，而是会——

1. 自己打开浏览器，搜索股票信息
2. 编写Python脚本，抓取实时股价数据
3. 调用金融API，获取公司财报信息
4. 用代码生成可视化图表
5. 最后整合成一份专业的分析报告

整个过程，它都在"边看边做边造工具"。

更惊人的是，基于Agent任务执行的**强化学习**，让AI在单次对话中可以**执行上百次工具调用**。要知道，以前的AI可能调用3-5次API就"累趴"了，而现在的智能体可以连续工作数小时，不断搜索、分析、验证、迭代，直到任务完美交付。

这就像一个不知疲倦的实习生，越干越起劲。

### 条件二：算力平权，AI"飞入寻常百姓家"

2025年被称为"AI Agent元年"的另一个关键原因是：**算力，不再是大厂的专利了**。

DeepSeek R1以OpenAI 1/10的成本实现了同等水平，直接引发了Token价格的"雪崩"——一年内下降了约90%。

这就像是手机行业的"小米时刻"：当曾经遥不可及的高端技术变得普惠，创新的闸门就会被彻底打开。

Manus和OpenClaw的出现，正是踩在这个历史性的时间点上。以前，只有科技巨头才能养得起这种"智能助手"；现在，普通人也能拥有自己的"贾维斯"了。

**技术的民主化，永远是创新最好的催化剂。**

---

## 三、三条进化路径：Manus、OpenClaw，还有千问

有趣的是，虽然都是智能体，但2025年我们实际上见证了**三条截然不同的发展路径**。它们各有特色，各显神通。

### Manus：从"超级实习生"到Meta的"20亿美元豪赌"

Manus采用了**多智能体架构（Multiple Agent）**。

这不是简单的"一个AI干所有活"，而是让多个专门的AI各司其职、相互协作。接到任务后，系统会自动分解成三个协作环节：

- **规划智能体（Planner）**：像项目经理一样，把复杂的大任务拆解成可执行的小步骤，制定执行路线图
- **执行智能体（Executor）**：像技术专员，动手操作电脑、编写代码、浏览网页、调用工具
- **验证智能体（Verifier）**：像质检员，检查结果是否正确，发现问题就要求返工

这三个"虚拟员工"在云端7×24小时不间断协作，从不知疲倦。

**技术架构上，Manus运行在云端虚拟机中。** 这意味着：
- 每个用户都有独立的"沙盒环境"
- AI可以安装软件、运行代码、操作浏览器——就像真正的实习生坐在电脑前
- 任务执行是**异步的**：你发送需求后可以关闭电脑，AI会继续工作，完成后通过邮件/消息通知你

但Manus与DeepSeek、ChatGPT等产品的核心差异在于：**它交付的是结果，而不是建议。**

#### "会搜索" vs "会执行"：本质的区别

DeepSeek、ChatGPT、Perplexity这些AI搜索产品，无论你问什么，它们最终给你的都是**一段文字答案**。

比如你说"帮我分析特斯拉的股票值不值得买"，它们会给你一份详细的分析报告——但仅此而已。

Manus不一样。它会：
1. 自己打开浏览器，搜索特斯拉最新的财报
2. 编写Python脚本，抓取股价历史数据
3. 调用金融API，获取分析师评级
4. 生成可视化图表，制作对比分析
5. 最后把完整的Excel报告+PDF分析发给你

**关键是：你可以在说完需求后，合上笔记本去睡觉。Manus会在云端虚拟机里默默工作，等你醒来时，成品已经在邮箱里了。**

这种"异步执行+交付成果"的能力，是Manus与所有"对话式AI"的本质区别。

#### 被Meta收购：一场20亿美元的豪赌

2025年底，Manus的故事迎来了戏剧性转折。

**2025年12月30日，Meta宣布以超过20亿美元收购Manus**——这距离Manus发布还不到一年，估值翻了4-6倍。

CNBC、Reuters、TechCrunch等权威媒体纷纷报道：Meta将把Manus整合进其AI产品线，包括WhatsApp for Business、Meta AI等。

扎克伯格的野心很明确：拿下这个"全球首款通用AI Agent"，补上Meta在AI执行层的短板。

#### 中国政府的审查：一场早有预演的"切割"

但这笔交易最戏剧性的，是后续的**地缘政治风波**。

Manus虽然注册在新加坡，但其创始团队有中国背景，核心技术也源于中国工程师。Meta收购消息一出，**中国商务部迅速宣布启动审查**。

审查的焦点是：这笔交易是否违反了中国的技术出口管制？

《南华早报》报道称，这被视为中国版CFIUS（外国投资委员会）的**高调测试案例**。但细究时间线会发现：这场"切割"其实早有预演。

**早在2025年6月，Manus就将总部从中国迁至新加坡**。当时官方说法是"为了更好地获得国际投资"，但实际上，这标志着与中国业务的切割已经开始。

Meta发言人的声明证实了这一点：收购完成后，Manus "would no longer have ongoing Chinese ownership interests and **would terminate its services and operations in China**"。

这场风波给所有中国AI创业者敲响了警钟：当你的技术足够先进时，它可能不再只是"技术"，而是"战略资产"。而"出海"这道选择题，可能比想象中来得更早、更决绝。

### OpenClaw：那个让Mac Mini卖断货的"龙虾"

如果说Manus是"云端实习生"，OpenClaw则是2025年末最疯狂的"草根革命者"。

它的故事，堪比一部硅谷创业大片。

#### 从"卖断货"到"入豪门"：OpenClaw的疯狂72小时

2026年1月底，OpenClaw（前身Clawdbot）在GitHub上开源后，以惊人的速度斩获**14.5万星标**——这让它成为GitHub历史上增长最快的项目之一。

但比这更疯狂的是：**它直接把苹果的Mac Mini和Mac Studio买断了货**。

事情是这样的：OpenClaw主打"本地优先"，所有AI运算都在用户自己的设备上完成。这意味着什么？意味着你需要一台性能强劲、内存充足的电脑来"供养"这个AI管家。而苹果M4芯片的Mac Mini（尤其是16GB内存版本），以其性价比和功耗比，成为极客们的首选。

结果呢？**全球范围内的Mac Mini被抢购一空**。发货时间从原来的14天延长到54天，某些配置甚至需要等待6周。Tom's Hardware等科技媒体直接 headlines："OpenClaw-fueled ordering frenzy creates Apple Mac shortage"（OpenClaw引发的抢购潮导致苹果缺货）。

但这还不是最戏剧性的。

**2026年2月15日，也就是昨天，OpenAI CEO Sam Altman在X上宣布：OpenClaw创始人Peter Steinberger正式加入OpenAI。**

没错，那个穿着睡衣、在家里车库写出Clawdbot的独立开发者，现在成为了OpenAI的一员。Altman说，Peter将"drive the next generation of personal agents"（驱动下一代个人智能体），而OpenClaw本身将以开源基金会的形式继续存在，由OpenAI提供支持。

从GitHub星标爆发到被OpenAI"招安"，OpenClaw用了不到一个月。这种火箭般的上升速度，在AI发展史上也属罕见。

#### 飞书里的"贾维斯"：不只是本地，还能远程

OpenClaw另一个让国内用户兴奋的功能是：**它原生支持飞书（Lark）接入**。

这意味着什么？

想象一下：你在飞书上@你的AI助手，说"帮我分析一下这个月销售数据，写到飞书文档里"。5分钟后，飞书文档里出现了一份图文并茂的分析报告——**整个过程，AI是在你自己的Mac Mini上运行的，数据从未离开你的本地网络**。

这种"本地计算 + 远程交互"的模式，既保证了隐私安全，又实现了随时随地的访问。你甚至可以在地铁上用手机发消息，让家里的AI帮你整理资料、写代码、查数据。

#### Skill+SOP：AI的"操作手册"

OpenClaw最独特的技术创新，是它的**Skill（技能）系统**。

你可以把Skill理解成**SOP（标准操作流程）**——就像公司里的《员工操作手册》，但这份手册是给AI看的。

普通的AI对话是这样的：
> 你：帮我查一下天气
> AI：查哪个城市？用什么网站？摄氏度还是华氏度？

每次都像教新员工，累死。

但有了Skill，你给AI准备了一份**《查天气标准操作手册》**（一个Markdown文件）：
- **触发条件**：当用户问天气相关问题时
- **执行步骤**：调用 `curl wttr.in/城市名` 命令获取数据
- **输出格式**：提取温度、天气状况、风速，用口语化方式回复
- **异常处理**：如果城市名不明确，询问用户具体位置

这份手册（SKILL.md）不仅包含**提示词（prompt）**，更重要的是还包含**可执行的代码/脚本**。OpenClaw把这份SOP"教"给AI，AI就知道该调用哪个命令、怎么处理结果、遇到问题怎么办。

想添加新能力？只要会写Markdown、会写脚本，就像写一份新的SOP手册一样简单。查天气、发邮件、操作Excel、控制智能家居、甚至调用飞书API写文档……一切能写成脚本的东西，都能变成AI的工具。

这正是为什么OpenClaw如此强大：**它不是给你一个固定的AI，而是给你一个可以自己"调教"AI的框架。** 每个人的OpenClaw都是独一无二的，取决于你给它装了什么Skill。

### 千问：生态整合的"万能入口"

与前两者的技术路线不同，阿里在2025年底推出的**千问App**，走的是另一条路——**大厂生态整合**。

2025年11月，阿里将原有的"通义"App正式升级为"千问"，定位为**"个人AI助手和AI生活入口"**。

但千问最特别的地方，不是模型能力有多强，而是它**深度整合了阿里庞大的生态系统**——不只是"能调用"，而是真的能"动手办事"。

#### "动动嘴就搞定"的真实场景

根据《证券日报》等媒体的实测，千问能做到：

**点外卖场景**：
> 你对千问说"帮我点两杯拿铁，送到公司"
> 
> 千问立即调用**淘宝闪购**，选择最近的咖啡店，自动填写地址，通过**AI付**在端内完成支付——**全程无需跳转其他App**。

**出行场景**：
> 你说"春节想带家人去三亚玩5天，预算2万"
> 
> 千问调用**飞猪**查询机票酒店，调用**高德**规划每日行程，甚至能**直接打电话帮你预订年夜饭餐厅**。

**购物场景**：
> 你拍一张同事穿的衣服照片，千问识别后**直接跳转淘宝链接**，显示同款或相似款，一键下单。

这种**"识别→决策→执行→支付"的完整闭环**，是Manus和OpenClaw暂时无法做到的——因为它们没有自己的"支付+物流+本地生活"基础设施。

千问的背后是**通义千问Qwen2.5-Max**大模型，但它的核心竞争力在于：**它是阿里所有业务的统一AI交互界面**。

这种打法的关键在于——**生态闭环**。当Manus和OpenClaw还需要"现场造工具"时，千问直接接入了阿里沉淀20年的基础设施：支付（支付宝）、物流（菜鸟）、本地生活（饿了么、高德）、电商（淘宝、天猫）。

吴泳铭将千问定位为**"集团战略级项目"**，它不属于任何一个业务线，没有历史包袱，可以公平地调用所有阿里资源。这种"中立入口"的身份，让它有机会成为真正意义上的"**出门带一个App解决所有问题**"的超级入口。

---

## 四、多模态的"进化论"：从"抽卡出图"到"导演级AI"

2025年不仅是智能体的元年，也是**多模态AI**大爆发的一年。

你有没有注意到，现在的AI生成图片越来越"听话"了？想要一只戴着墨镜的赛博朋克猫咪？没问题，光影、角度、表情，都能精准控制。

这背后，是一场**从"直接生成"到"迭代优化"**的技术革命。

### 进化的三个阶段

**第一阶段（2024年前）**：Stable Diffusion等工具可以"一键出图"，但就像开盲盒——细节全靠运气，很难精准控制。

**第二阶段（2025年上半年）**：GPT-4o和Gemini 2.5 Flash引入**图片编辑能力**。你可以说"把这张图里的猫换成狗"，AI能理解并精准修改。这是一个质的飞跃。

**第三阶段（2025年底）**：谷歌的**Nano Banana**和OpenAI的GPT Image 1.5带来了真正的"智能体式生图"。

Nano Banana的强大之处在于：**它背后是一整套智能体工作流**。

当你说"生成一张赛博朋克风格的猫咪，戴着墨镜，背景是霓虹灯街道"，Nano Banana会——

1. **意图扩展**：深度理解你的需求，自动补充细节（猫咪的品种、墨镜的反光样式、霓虹灯的色调氛围）
2. **生图**：生成初稿
3. **图片理解与分析**：像专业摄影师一样审视作品，检查是否符合要求
4. **再次生图或局部编辑**：对不满意的部分进行精准调整

这种"生成→分析→再生成"的**迭代逻辑**，让AI生图从"抽卡游戏"变成了"精准创作"。

**技术突破点**：Nano Banana采用了**Agentic Loop（智能体循环）**架构——生成器（Generator）和评估器（Evaluator）相互博弈，直到结果达标。这跟人像摄影师反复调整灯光、模特反复摆pose直到满意是一样的逻辑。

### 视频领域的"Seedance冲击"

而在视频领域，字节跳动2025年6月推出的**Seedance**以及2026年初惊艳亮相的**Seedance 2.0**，正在复刻这一进化路径。

Seedance 2.0不仅能根据文字生成电影级视频，更能**精准复刻运镜逻辑、动作细节与音乐氛围**。

其核心技术包括：
- **时空一致性**：确保视频中人物在不同帧之间保持一致的外观和动作逻辑
- **运镜语言理解**：能识别"推轨镜头"、"环绕拍摄"、"慢动作"等专业术语并精准执行
- **音画同步**：生成的视频能与背景音乐的节奏、情绪完美匹配

许多专业创作者体验后感慨："一键生成电影"的时代，可能真的不远了。

---

## 五、为什么偏偏是2025年？

回顾这波澜壮阔的一年，AI完成了从"工具"到"伙伴"的三级跳：

### 第一步：长出大脑（DeepSeek R1）
- 学会"慢思考"，具备真正的推理和规划能力
- 基于CoT的AI搜索开始普及，AI开始"理解"而非"匹配"

### 第二步：长出手脚（Manus、OpenClaw、千问）
- 代码生成能力的突破，让AI可以"现场造工具"
- 单次prompt可执行上百次工具调用，效率指数级提升
- 从"给建议"进化为"交付结果"
- **三条路径并进**：Manus做云端全能助手，OpenClaw做本地隐私管家，千问做大厂生态入口

### 第三步：全面感知（GPT-4o、Nano Banana、Seedance 2.0）
- 多模态智能体实现"意图扩展→生成→分析→再生成"的迭代闭环
- AI开始具备"导演级"的创作能力

### 背后的双重逻辑

这背后，是两个底层逻辑的成熟：

**1. 技术逻辑**：后训练（Post-training）+ 强化学习让模型能力产生质变，AI开始真正"学会学习"。

**2. 经济逻辑**：算力平权让高端AI从"奢侈品"变成"日用品"，创新的门槛被彻底打破。

---

## 尾声：这只是开始

2025年的这些突破，让我们离真正的AGI（通用人工智能）更近了一步。

也许不久之后，每个人的手机里都会住着一个"贾维斯"——

它知道你的一切习惯，能帮你处理繁琐事务，甚至在你迷茫时给出贴心的建议。它不会疲倦，不会抱怨，永远在那里，随时待命。

这波澜壮阔的一年，**只是开始**。

未来已来，而你我，正身处其中。

---

*（本文约2600字，阅读时间约8分钟）*